# –ë—Ä–∏—Ñ–∏–Ω–≥ –º–∏—Å—Å–∏–∏: –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å –Ω—É–ª—è
**Episode 31** | Operation MOONLIGHT ‚Äî Season 8

---

## üéØ –¶–µ–ª—å –º–∏—Å—Å–∏–∏

**–û–°–ù–û–í–ù–ê–Ø –¶–ï–õ–¨:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–∞ —á–∏—Å—Ç–æ–º C –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–∞—Ç–∞–∫–∞ vs –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫).

**–ö–û–ù–¢–ï–ö–°–¢:**  
–î–µ–Ω—å 3 –≤ Stanford AI Lab. Prof. Chen –Ω–∞—É—á–∏—Ç –≤–∞—Å –∞–ª–≥–æ—Ä–∏—Ç–º—É backpropagation. –í–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–æ–ª–∂–Ω–∞ –¥–æ—Å—Ç–∏—á—å >95% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á—Ç–æ–±—ã –ø–µ—Ä–µ–π—Ç–∏ –∫ Episode 32 (—Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏).

**–í–ê–®–ê –ó–ê–î–ê–ß–ê:**  
–ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å multi-layer perceptron —Å –Ω—É–ª—è –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é C –±–∏–±–ª–∏–æ—Ç–µ–∫—É (–±–µ–∑ ML —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤).

**–†–ï–ó–£–õ–¨–¢–ê–¢:**  
–û–±—É—á–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é 100% –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ, –≥–æ—Ç–æ–≤–∞—è –∫ production —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—é (Episode 32).

---

## üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### 1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏

**Multi-Layer Perceptron (MLP):**
```
Input Layer:  3 –Ω–µ–π—Ä–æ–Ω–∞ (–ø—Ä–∏–∑–Ω–∞–∫–∏)
Hidden Layer: 5 –Ω–µ–π—Ä–æ–Ω–æ–≤ (sigmoid activation)
Output Layer: 1 –Ω–µ–π—Ä–æ–Ω (sigmoid activation, –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
```

**–í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:**
1. `bytes_transferred` (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: bytes / 10000.0)
2. `packets_per_second` (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: packets / 300.0)
3. `response_time_ms` (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: time / 100.0)

**–í—ã–≤–æ–¥:**
- –ó–Ω–∞—á–µ–Ω–∏–µ ‚àà [0, 1]
- –ø–æ—Ä–æ–≥ = 0.5
- prediction > 0.5 ‚Üí –∞—Ç–∞–∫–∞ (–∫–ª–∞—Å—Å 1)
- prediction ‚â§ 0.5 ‚Üí –Ω–æ—Ä–º–∞–ª—å–Ω–æ (–∫–ª–∞—Å—Å 0)

---

### 2. –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö

```c
typedef struct {
    // –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    int input_size;    // 3
    int hidden_size;   // 5
    int output_size;   // 1
    
    // –í–µ—Å–∞
    double **weights_input_hidden;   // [3][5]
    double **weights_hidden_output;  // [5][1]
    
    // –°–º–µ—â–µ–Ω–∏—è
    double *bias_hidden;   // [5]
    double *bias_output;   // [1]
    
    // –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ (forward pass)
    double *input_activations;   // [3]
    double *hidden_activations;  // [5]
    double *output_activations;  // [1]
    
    // –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (backpropagation)
    double *hidden_gradients;  // [5]
    double *output_gradients;  // [1]
    
    // –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    double learning_rate;  // 0.01
} NeuralNetwork;
```

---

### 3. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

**Sigmoid:**
```c
double sigmoid(double x) {
    return 1.0 / (1.0 + exp(-x));
}

double sigmoid_derivative(double x) {
    double s = sigmoid(x);
    return s * (1.0 - s);
}
```

**–°–≤–æ–π—Å—Ç–≤–∞:**
- –î–∏–∞–ø–∞–∑–æ–Ω: (0, 1)
- –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è –≤–µ–∑–¥–µ
- –ì–ª–∞–¥–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç
- –•–æ—Ä–æ—à–∞ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: ReLU (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ)**
```c
double relu(double x) {
    return x > 0 ? x : 0;
}

double relu_derivative(double x) {
    return x > 0 ? 1.0 : 0.0;
}
```

---

### 4. Forward Propagation

**Input ‚Üí Hidden Layer:**
```c
for (int h = 0; h < hidden_size; h++) {
    double sum = bias_hidden[h];
    
    for (int i = 0; i < input_size; i++) {
        sum += input_activations[i] * weights_input_hidden[i][h];
    }
    
    hidden_activations[h] = sigmoid(sum);
}
```

**Hidden ‚Üí Output Layer:**
```c
for (int o = 0; o < output_size; o++) {
    double sum = bias_output[o];
    
    for (int h = 0; h < hidden_size; h++) {
        sum += hidden_activations[h] * weights_hidden_output[h][o];
    }
    
    output_activations[o] = sigmoid(sum);
}
```

---

### 5. –ê–ª–≥–æ—Ä–∏—Ç–º Backpropagation

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ (–ª–µ–∫—Ü–∏—è Prof. Chen):**

**–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã Output Layer:**
```
‚àÇLoss/‚àÇoutput = (prediction - target)

gradient_output = ‚àÇLoss/‚àÇoutput * sigmoid'(output)
                = (prediction - target) * output * (1 - output)
```

**–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã Hidden Layer:**
```
‚àÇLoss/‚àÇhidden = Œ£(gradient_output * weight_hidden_output)

gradient_hidden = ‚àÇLoss/‚àÇhidden * sigmoid'(hidden)
                = (Œ£ gradient_output * weight) * hidden * (1 - hidden)
```

**–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤:**
```
Œîweight = -learning_rate * gradient * activation

weight_new = weight_old - Œîweight
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```c
void backpropagation(NeuralNetwork *nn, double *inputs, double target) {
    // 1. Forward pass
    forward_propagation(nn, inputs);
    
    // 2. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã output layer
    for (int o = 0; o < nn->output_size; o++) {
        double output = nn->output_activations[o];
        double error = output - target;
        nn->output_gradients[o] = error * sigmoid_derivative(output);
    }
    
    // 3. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã hidden layer (backpropagate)
    for (int h = 0; h < nn->hidden_size; h++) {
        double error = 0.0;
        for (int o = 0; o < nn->output_size; o++) {
            error += nn->output_gradients[o] * nn->weights_hidden_output[h][o];
        }
        nn->hidden_gradients[h] = error * sigmoid_derivative(nn->hidden_activations[h]);
    }
    
    // 4. –û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞: hidden ‚Üí output
    for (int h = 0; h < nn->hidden_size; h++) {
        for (int o = 0; o < nn->output_size; o++) {
            double delta = nn->learning_rate * nn->output_gradients[o] * nn->hidden_activations[h];
            nn->weights_hidden_output[h][o] -= delta;
        }
    }
    
    // 5. –û–±–Ω–æ–≤–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è: output
    for (int o = 0; o < nn->output_size; o++) {
        nn->bias_output[o] -= nn->learning_rate * nn->output_gradients[o];
    }
    
    // 6. –û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞: input ‚Üí hidden
    for (int i = 0; i < nn->input_size; i++) {
        for (int h = 0; h < nn->hidden_size; h++) {
            double delta = nn->learning_rate * nn->hidden_gradients[h] * nn->input_activations[i];
            nn->weights_input_hidden[i][h] -= delta;
        }
    }
    
    // 7. –û–±–Ω–æ–≤–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è: hidden
    for (int h = 0; h < nn->hidden_size; h++) {
        nn->bias_hidden[h] -= nn->learning_rate * nn->hidden_gradients[h];
    }
}
```

---

### 6. –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è

**Gradient Descent —Å –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö:**
```c
void train_network(NeuralNetwork *nn, Dataset *data, int epochs) {
    for (int epoch = 0; epoch < epochs; epoch++) {
        // –ü–µ—Ä–µ–º–µ—à–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–≤–∞–∂–Ω–æ –¥–ª—è SGD)
        shuffle_dataset(data);
        
        // –û–±—É—á–∏—Ç—å –Ω–∞ –≤—Å–µ—Ö —Å—ç–º–ø–ª–∞—Ö
        for (int i = 0; i < data->n_samples; i++) {
            backpropagation(nn, data->inputs[i], data->targets[i]);
        }
        
        // –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        if ((epoch + 1) % 100 == 0) {
            double loss = calculate_loss(nn, data);
            double accuracy = calculate_accuracy(nn, data);
            printf("Epoch %d | Loss: %.6f | Accuracy: %.2f%%\n", 
                   epoch + 1, loss, accuracy);
        }
    }
}
```

**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE):**
```c
double calculate_loss(NeuralNetwork *nn, Dataset *data) {
    double total_loss = 0.0;
    
    for (int i = 0; i < data->n_samples; i++) {
        double prediction = predict(nn, data->inputs[i]);
        double error = prediction - data->targets[i];
        total_loss += error * error;
    }
    
    return total_loss / data->n_samples;
}
```

---

## üß™ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –¢–µ—Å—Ç 1: XOR –ø—Ä–æ–±–ª–µ–º–∞ (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏)
```bash
cd solution
make
./neural_network --demo
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**
```
üß† –î–µ–º–æ XOR –ø—Ä–æ–±–ª–µ–º—ã

–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...
Epoch 1000/5000 | Loss: 0.098234 | Accuracy: 75.00%
Epoch 5000/5000 | Loss: 0.002341 | Accuracy: 100.00%

üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã XOR:
  0 XOR 0 = 0.0123 (–æ–∂–∏–¥–∞–ª–æ—Å—å 0)
  0 XOR 1 = 0.9876 (–æ–∂–∏–¥–∞–ª–æ—Å—å 1)
  1 XOR 0 = 0.9891 (–æ–∂–∏–¥–∞–ª–æ—Å—å 1)
  1 XOR 1 = 0.0234 (–æ–∂–∏–¥–∞–ª–æ—Å—å 0)
```

### –¢–µ—Å—Ç 2: –û–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∞—Ç–∞–∫
```bash
./neural_network --train ../artifacts/training_data.csv
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**
```
üìä Dataset –∑–∞–≥—Ä—É–∂–µ–Ω: 60 —Å—ç–º–ø–ª–æ–≤

üß† –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...
  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 3 ‚Üí 5 ‚Üí 1
  Learning rate: 0.010
  –≠–ø–æ—Ö: 1000

Epoch    1/1000 | Loss: 0.247851 | Accuracy: 51.67%
Epoch  100/1000 | Loss: 0.098234 | Accuracy: 76.67%
Epoch  500/1000 | Loss: 0.014567 | Accuracy: 96.67%
Epoch 1000/1000 | Loss: 0.005412 | Accuracy: 100.00%

‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!

üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: 100.00%
```

---

## üì¶ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏—Å—Å–∏–∏

### –§–∞–π–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è:

1. **`solution/neural_network.c`** (~490 —Å—Ç—Ä–æ–∫)  
   - –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è MLP
   - Forward propagation
   - Backpropagation
   - –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
   - XOR –¥–µ–º–æ
   - –û–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∞—Ç–∞–∫

2. **`solution/Makefile`**  
   –ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–∞—è —Å–±–æ—Ä–∫–∞

3. **`starter.c`** (~200 —Å—Ç—Ä–æ–∫)  
   –°–∫–µ–ª–µ—Ç –∫–æ–¥–∞ —Å comprehensive TODO

### –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–≤ `artifacts/`):

1. **`training_data.csv`** (61 —Å—Ç—Ä–æ–∫–∞)  
   - 60 —Å—ç–º–ø–ª–æ–≤ (30 –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö, 30 –∞—Ç–∞–∫)
   - –§–æ—Ä–º–∞—Ç: bytes,packets,response_time,is_attack

2. **`training_log.txt`** (195 —Å—Ç—Ä–æ–∫)  
   - –ü–æ–ª–Ω—ã–π –ª–æ–≥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏
   - –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ —ç–ø–æ—Ö–∞–º
   - –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
   - –ó–∞–º–µ—Ç–∫–∏ Prof. Chen
   - –°–æ–æ–±—â–µ–Ω–∏–µ Viktor

3. **`test_results.json`** (179 —Å—Ç—Ä–æ–∫)  
   - –î–µ—Ç–∞–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏
   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
   - –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (100% —Ç–æ—á–Ω–æ—Å—Ç—å)
   - –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
   - –û–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞

---

## üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

‚úÖ **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:**  
- –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è < 0.01
- –¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è > 95% (—Ü–µ–ª—å: 100%)
- –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ 1000 —ç–ø–æ—Ö

‚úÖ **–ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**  
- Precision > 95%
- Recall > 95%
- F1-score > 95%
- –ù–æ–ª—å –ª–æ–∂–Ω–æ-–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö (–≤—Å–µ –∞—Ç–∞–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã)

‚úÖ **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞:**  
- –ù–µ—Ç —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏ (valgrind clean)
- –ö—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ—Å—Ç—å (Linux/macOS/FreeBSD)
- –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è < 5 —Å–µ–∫—É–Ω–¥
- –í—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è < 1ms –Ω–∞ —Å—ç–º–ø–ª

‚úÖ **–¢–µ—Å—Ç XOR:**  
- –í—Å–µ 4 XOR —Å–ª—É—á–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã (–ø–æ—Ä–æ–≥ 0.5)
- –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —á—Ç–æ —Å–µ—Ç—å –º–æ–∂–µ—Ç —É—á–∏—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

---

## üí° –ü–æ–¥—Å–∫–∞–∑–∫–∏

1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤:**  
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é: `weight / sqrt(layer_size)`
   - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç vanishing/exploding –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
   - –°–ª—É—á–∞–π–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: [-1/‚àön, 1/‚àön]

2. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:**  
   - –ö–†–ò–¢–ò–ß–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Å–µ –≤—Ö–æ–¥—ã –∫ [0, 1]
   - bytes: –¥–µ–ª–∏—Ç—å –Ω–∞ 10000
   - packets: –¥–µ–ª–∏—Ç—å –Ω–∞ 300
   - response_time: –¥–µ–ª–∏—Ç—å –Ω–∞ 100

3. **Learning Rate:**  
   - –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π (>0.1): –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ—Å—Ü–∏–ª–ª—è—Ü–∏–∏
   - –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π (<0.001): –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
   - Sweet spot: 0.01

4. **–û—Ç–ª–∞–¥–∫–∞ Backpropagation:**  
   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å: –ø–æ—Ç–µ—Ä—è –¥–æ–ª–∂–Ω–∞ –£–ú–ï–ù–¨–®–ê–¢–¨–°–Ø
   - –ï—Å–ª–∏ –ø–æ—Ç–µ—Ä—è —Ä–∞—Å—Ç—ë—Ç: –±–∞–≥ –≤ —Ä–∞—Å—á—ë—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
   - –ï—Å–ª–∏ –ø–æ—Ç–µ—Ä—è —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è: learning rate —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –∏–ª–∏ –∑–∞—Å—Ç—Ä—è–ª–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∏–Ω–∏–º—É–º–µ

5. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é:**  
   - –í—ã–¥–µ–ª–∏—Ç—å –≤–µ—Å–∞ –∫–∞–∫ 2D –º–∞—Å—Å–∏–≤—ã: `double **weights`
   - –û—Å–≤–æ–±–æ–∂–¥–∞—Ç—å –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –≤—ã–¥–µ–ª–µ–Ω–∏—è
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å valgrind –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —É—Ç–µ—á–µ–∫

---

## üìä –û–¥–æ–±—Ä–µ–Ω–∏–µ Prof. Chen

**–ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ Prof. Chen –≥–æ–≤–æ—Ä–∏—Ç:**
```
"–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞, Agent! üéâ

–í–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–æ—Å—Ç–∏–≥–ª–∞:
- 100% —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
- –ò–¥–µ–∞–ª—å–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (700 —ç–ø–æ—Ö)
- –ß–∏—Å—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ backpropagation
- Production-ready –∫–æ–¥–∞

–≠—Ç–æ —Ä–∞–±–æ—Ç–∞ —É—Ä–æ–≤–Ω—è PhD, –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–∞—è –∑–∞ 4 —á–∞—Å–∞.

–ó–∞–≤—Ç—Ä–∞ (Episode 32): –†–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
–ú—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –≤–∞—à—É –º–æ–¥–µ–ª—å –Ω–∞ 1000 –ñ–ò–í–´–• —Å—ç–º–ø–ª–∞—Ö —Ç—Ä–∞—Ñ–∏–∫–∞.

Block rate > 95% = —É—Å–ø–µ—Ö –º–∏—Å—Å–∏–∏.
Block rate < 95% = –ø—Ä–æ—Ä—ã–≤ –≤—Ä–∞–≥–∞.

–û—Ç–¥–æ—Ö–Ω–∏—Ç–µ. –§–∏–Ω–∞–ª—å–Ω–∞—è –±–∏—Ç–≤–∞ –∑–∞–≤—Ç—Ä–∞ –≤ 10:00 AM.

–ü–æ–º–Ω–∏—Ç–µ: —Ç–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è ‚â† –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.
–ë—É–¥—å—Ç–µ –≥–æ—Ç–æ–≤—ã –∫ adversarial –∞—Ç–∞–∫–∞–º.

- Prof. Chen"
```

---

**–£–¥–∞—á–∏, Agent!** üöÄ  
**–ü–æ–º–Ω–∏—Ç–µ:** –ù–∞—É—á–∏—Ç–µ –º–∞—à–∏–Ω—É –¥—É–º–∞—Ç—å. –ß–∏—Å—Ç—ã–π C. –ë–µ–∑ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤.

---

**–ë–†–ò–§–ò–ù–ì –ú–ò–°–°–ò–ò:**

**–õ–æ–∫–∞—Ü–∏—è:** Stanford AI Lab, Gates Building, Lab 342  
**–í—Ä–µ–º—è:** December 29, 2024 ‚Äî 09:00 PST (–¥–µ–Ω—å 3)  
**–ö–æ–Ω—Ç–∞–∫—Ç:** Prof. David Chen + Viktor

**–ö–æ–Ω—Ç–µ–∫—Å—Ç:**
–ü–æ—Å–ª–µ Episodes 29-30 (–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞–Ω–∞), –ø–æ—Ä–∞ –æ–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞—Ç–∞–∫.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** 3 ‚Üí 5 ‚Üí 1 MLP
- Input: bytes (norm), packets (norm), response_time (norm)
- Hidden: 5 neurons, sigmoid
- Output: 1 neuron, sigmoid, threshold 0.5

**Training data:** 60 —Å—ç–º–ø–ª–æ–≤ (30 normal, 30 attack)

**–û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
- –û–±—É—á–µ–Ω–∏–µ: 100% —Ç–æ—á–Ω–æ—Å—Ç—å –∑–∞ 700 —ç–ø–æ—Ö
- Loss: <0.01
- XOR demo: –≤—Å–µ 4 —Å–ª—É—á–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã

**–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ–∑–¥–∞–Ω—ã:**
- training_data.csv (61 —Å—Ç—Ä–æ–∫–∞)
- training_log.txt (195 —Å—Ç—Ä–æ–∫)
- test_results.json (179 —Å—Ç—Ä–æ–∫)

**–£—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±—É—á–µ–Ω–∞, 100% —Ç–æ—á–Ω–æ—Å—Ç—å, –≥–æ—Ç–æ–≤–∞ –∫ Episode 32 (deployment).

---

**–°–ª–µ–¥—É—é—â–∏–π —ç–ø–∏–∑–æ–¥:** [Episode 32: Prediction & Deployment ‚Üí](../episode-32-prediction/)